{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import surprise\n",
    "from sklearn.model_selection import train_test_split#, cross_validate\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import KNNWithMeans, SVD, SVDpp, SlopeOne\n",
    "from surprise import accuracy\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy\n",
    "from surprise import NormalPredictor\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNWithZScore\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD\n",
    "from surprise import BaselineOnly\n",
    "from surprise import SVDpp\n",
    "from surprise import NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv('goodbooks-10k/books.csv')\n",
    "ratings = pd.read_csv('goodbooks-10k/ratings.csv')\n",
    "toread = pd.read_csv('goodbooks-10k/to_read.csv')\n",
    "tags = pd.read_csv('goodbooks-10k/tags.csv')\n",
    "book_tags = pd.read_csv('goodbooks-10k/book_tags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOKS : \n",
      " (10000, 23) \n",
      "    id  book_id  best_book_id  work_id  books_count       isbn        isbn13  \\\n",
      "0   1  2767052       2767052  2792775          272  439023483  9.780439e+12   \n",
      "1   2        3             3  4640799          491  439554934  9.780440e+12   \n",
      "2   3    41865         41865  3212258          226  316015849  9.780316e+12   \n",
      "3   4     2657          2657  3275794          487   61120081  9.780061e+12   \n",
      "4   5     4671          4671   245494         1356  743273567  9.780743e+12   \n",
      "\n",
      "                       authors  original_publication_year  \\\n",
      "0              Suzanne Collins                     2008.0   \n",
      "1  J.K. Rowling, Mary GrandPré                     1997.0   \n",
      "2              Stephenie Meyer                     2005.0   \n",
      "3                   Harper Lee                     1960.0   \n",
      "4          F. Scott Fitzgerald                     1925.0   \n",
      "\n",
      "                             original_title  ... ratings_count  \\\n",
      "0                          The Hunger Games  ...       4780653   \n",
      "1  Harry Potter and the Philosopher's Stone  ...       4602479   \n",
      "2                                  Twilight  ...       3866839   \n",
      "3                     To Kill a Mockingbird  ...       3198671   \n",
      "4                          The Great Gatsby  ...       2683664   \n",
      "\n",
      "  work_ratings_count  work_text_reviews_count  ratings_1  ratings_2  \\\n",
      "0            4942365                   155254      66715     127936   \n",
      "1            4800065                    75867      75504     101676   \n",
      "2            3916824                    95009     456191     436802   \n",
      "3            3340896                    72586      60427     117415   \n",
      "4            2773745                    51992      86236     197621   \n",
      "\n",
      "   ratings_3  ratings_4  ratings_5  \\\n",
      "0     560092    1481305    2706317   \n",
      "1     455024    1156318    3011543   \n",
      "2     793319     875073    1355439   \n",
      "3     446835    1001952    1714267   \n",
      "4     606158     936012     947718   \n",
      "\n",
      "                                           image_url  \\\n",
      "0  https://images.gr-assets.com/books/1447303603m...   \n",
      "1  https://images.gr-assets.com/books/1474154022m...   \n",
      "2  https://images.gr-assets.com/books/1361039443m...   \n",
      "3  https://images.gr-assets.com/books/1361975680m...   \n",
      "4  https://images.gr-assets.com/books/1490528560m...   \n",
      "\n",
      "                                     small_image_url  \n",
      "0  https://images.gr-assets.com/books/1447303603s...  \n",
      "1  https://images.gr-assets.com/books/1474154022s...  \n",
      "2  https://images.gr-assets.com/books/1361039443s...  \n",
      "3  https://images.gr-assets.com/books/1361975680s...  \n",
      "4  https://images.gr-assets.com/books/1490528560s...  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "RATINGS : \n",
      " (981756, 3) \n",
      "    book_id  user_id  rating\n",
      "0        1      314       5\n",
      "1        1      439       3\n",
      "2        1      588       5\n",
      "3        1     1169       4\n",
      "4        1     1185       4\n",
      "TO READ : \n",
      " (912705, 2) \n",
      "    user_id  book_id\n",
      "0        1      112\n",
      "1        1      235\n",
      "2        1      533\n",
      "3        1     1198\n",
      "4        1     1874\n",
      "TAGS : \n",
      " (34252, 2) \n",
      "        tag_id    tag_name\n",
      "34247   34247   Ｃhildrens\n",
      "34248   34248   Ｆａｖｏｒｉｔｅｓ\n",
      "34249   34249       Ｍａｎｇａ\n",
      "34250   34250      ＳＥＲＩＥＳ\n",
      "34251   34251  ｆａｖｏｕｒｉｔｅｓ\n",
      "BOOK_TAGS : \n",
      " (999912, 3) \n",
      "    goodreads_book_id  tag_id   count\n",
      "0                  1   30574  167697\n",
      "1                  1   11305   37174\n",
      "2                  1   11557   34173\n",
      "3                  1    8717   12986\n",
      "4                  1   33114   12716\n"
     ]
    }
   ],
   "source": [
    "print('BOOKS :', '\\n', books.shape, '\\n', books.head())\n",
    "print('RATINGS :', '\\n', ratings.shape, '\\n', ratings.head())\n",
    "print('TO READ :', '\\n', toread.shape, '\\n', toread.head())\n",
    "print('TAGS :', '\\n', tags.shape, '\\n', tags.tail())\n",
    "print('BOOK_TAGS :', '\\n', book_tags.shape , '\\n',  book_tags.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before :  981756\n",
      "After dropping duplicates:  980112\n",
      "The number of unique users we have is: 53424\n",
      "The number of unique books we have is: 10000\n",
      "The median user rated 8 books.\n",
      "The max rating is: 5,  and the min rating is: 1\n"
     ]
    }
   ],
   "source": [
    "print('Before : ', ratings.shape[0])\n",
    "ratings.drop_duplicates(inplace=True)\n",
    "print('After dropping duplicates: ', ratings.shape[0])\n",
    "print('The number of unique users we have is:', len(ratings.user_id.unique()))\n",
    "print('The number of unique books we have is:', len(ratings.book_id.unique()))\n",
    "print(\"The median user rated %d books.\"%ratings.user_id.value_counts().median())\n",
    "print('The max rating is: %d,'%ratings.rating.max(),\" and the min rating is: %d\"%ratings.rating.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ratings[['user_id','book_id','rating']]\n",
    "dataset.columns = ['user','item','rating']\n",
    "dataset = dataset[0:100000]\n",
    "\n",
    "train_split, test_split = train_test_split(dataset, test_size=0.25)\n",
    "\n",
    "reader = surprise.Reader(rating_scale=(1,5))\n",
    "train_data = surprise.Dataset.load_from_df(train_split, reader)\n",
    "test_data = surprise.Dataset.load_from_df(test_split, reader)\n",
    "\n",
    "# collab_knn.get_neighbors(50, 5)    # .get_neighbors() returns closest items to given item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build user profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_profile_helper(item_id, item_ids, tfidf_matrix):\n",
    "    idx = item_ids.index(item_id)\n",
    "    item_profile = tfidf_matrix[idx:idx+1]\n",
    "    return item_profile\n",
    "\n",
    "\n",
    "def get_item_profiles(ids, item_ids, tfidf_matrix):\n",
    "    item_profiles_list = [item_profile_helper(x, item_ids, tfidf_matrix) for x in ids]\n",
    "    item_profiles = scipy.sparse.vstack(item_profiles_list)\n",
    "    return item_profiles\n",
    "\n",
    "\n",
    "def user_profile_helper(user_id, train_indexed, item_ids, tfidf_matrix):\n",
    "    '''\n",
    "    Builds user profile for a single user\n",
    "    '''\n",
    "    train_user_df = train_indexed.loc[user_id]\n",
    "    user_item_profiles = get_item_profiles(pd.Series(train_user_df['item']), item_ids, tfidf_matrix)\n",
    "    user_item_strengths = np.array(train_user_df['rating']).reshape(-1,1)\n",
    "    \n",
    "    #Weighted average of item profiles by the interactions strength\n",
    "    user_item_strengths_weighted_avg = np.sum(user_item_profiles.multiply(user_item_strengths), axis=0) / np.sum(user_item_strengths)\n",
    "    user_profile_norm = normalize(user_item_strengths_weighted_avg)\n",
    "    return user_profile_norm\n",
    "\n",
    "\n",
    "def build_users_profiles(item_ids, tfidf_matrix): \n",
    "    train_indexed = train_split[train_split['item'].isin(books.index+1)].set_index('user')\n",
    "    user_profiles = {}\n",
    "    for user_id in train_indexed.index.unique():\n",
    "        user_profiles[user_id] = user_profile_helper(user_id, train_indexed, item_ids, tfidf_matrix)\n",
    "        \n",
    "    return user_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend books based on tags\n",
    "\n",
    "- A user profile is built for each user.\n",
    "- Similarity between user profile and base (tfidf matrix) is calculated to get similar items to a user profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedRecommender():\n",
    "    \n",
    "    def __init__(self, books, book_tags, tags):\n",
    "        self.model_name = 'Content-Based-Recommender'\n",
    "        self.books = books\n",
    "        self.titles = books['title']\n",
    "        self.indices = pd.Series(books.index, index=books['title'])\n",
    "        self.tags_joined = pd.merge(book_tags, tags, \n",
    "                                    left_on='tag_id', right_on='tag_id', how='inner')\n",
    "        self.books_with_tags = pd.merge(books, self.tags_joined, \n",
    "                                        left_on='book_id', right_on='goodreads_book_id', how='inner')\n",
    "        self.item_ids  = self.get_item_ids()\n",
    "        self.tfidf_matrix = self.get_tfidf()\n",
    "        self.user_profiles = build_users_profiles(self.item_ids, self.tfidf_matrix)\n",
    "\n",
    "    \n",
    "    def get_item_ids(self):\n",
    "        self.item_ids = (self.books['id']).tolist()\n",
    "        return self.item_ids\n",
    "    \n",
    "    \n",
    "    def get_tfidf(self):\n",
    "        '''\n",
    "        TF-IDF tells us how important a word is \n",
    "        to a document in a collection\n",
    "        '''\n",
    "        tf = TfidfVectorizer(analyzer='word',\n",
    "                             ngram_range=(1, 2),\n",
    "                             min_df=0,\n",
    "                             max_features=1000,\n",
    "                             stop_words='english')\n",
    "        tfidf_matrix = tf.fit_transform(self.books_with_tags['tag_name'].head(10000))\n",
    "        return tfidf_matrix\n",
    "    \n",
    "    \n",
    "    def get_cosine_similarity(self, tfidf_matrix1, tfidf_matrix2):\n",
    "        print(tfidf_matrix1.shape)  #user_profile\n",
    "        print(tfidf_matrix2.shape)  #base tfidf\n",
    "        cosine_sim = linear_kernel(tfidf_matrix1, tfidf_matrix2)\n",
    "        \n",
    "        return cosine_sim\n",
    "    \n",
    "    \n",
    "    def _get_similar_items_to_user_profile(self, user_id, topn=1000):\n",
    "        cosine_similarities = self.get_cosine_similarity(self.user_profiles[user_id], self.tfidf_matrix)\n",
    "        similar_indices = cosine_similarities.argsort().flatten()[-topn:]\n",
    "        similar_items = sorted([(self.item_ids[i], cosine_similarities[0,i]) for i in similar_indices], \n",
    "                               key=lambda x: -x[1])\n",
    "        return similar_items\n",
    "    \n",
    "    \n",
    "    def get_recommendations(self, user_id, topn=1000):\n",
    "        similar_items = self._get_similar_items_to_user_profile(user_id)\n",
    "        \n",
    "        #Ignores items the user has already interacted with --- LATER\n",
    "        \n",
    "        recommendations_df = pd.DataFrame(similar_items, columns=['book_id', 'rating']).head(topn)\n",
    "        recommendations_df = recommendations_df.merge(self.books, how = 'left', \n",
    "                                                          left_on = 'book_id', \n",
    "                                                          right_on = 'id')[['id', 'original_title', 'rating']]\n",
    "        return recommendations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n",
      "(10000, 1000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_title</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7326</td>\n",
       "      <td>Doctors</td>\n",
       "      <td>0.326274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6306</td>\n",
       "      <td>Memories of Midnight</td>\n",
       "      <td>0.326274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7314</td>\n",
       "      <td>カードキャプターさくら 7 [Cardcaptor Sakura 7]</td>\n",
       "      <td>0.326274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.326274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2719</td>\n",
       "      <td>The Sorcerer in the North</td>\n",
       "      <td>0.326274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>4310</td>\n",
       "      <td>The Lords of the North</td>\n",
       "      <td>0.238234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>8806</td>\n",
       "      <td>Warlock: A Novel of Ancient Egypt</td>\n",
       "      <td>0.238234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1208</td>\n",
       "      <td>Four Past Midnight</td>\n",
       "      <td>0.238234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>5812</td>\n",
       "      <td>Ser Como o Rio que Flui</td>\n",
       "      <td>0.238234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>5711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.238234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                       original_title    rating\n",
       "0    7326                              Doctors  0.326274\n",
       "1    6306                 Memories of Midnight  0.326274\n",
       "2    7314  カードキャプターさくら 7 [Cardcaptor Sakura 7]  0.326274\n",
       "3    6323                                  NaN  0.326274\n",
       "4    2719            The Sorcerer in the North  0.326274\n",
       "..    ...                                  ...       ...\n",
       "995  4310               The Lords of the North  0.238234\n",
       "996  8806    Warlock: A Novel of Ancient Egypt  0.238234\n",
       "997  1208                   Four Past Midnight  0.238234\n",
       "998  5812              Ser Como o Rio que Flui  0.238234\n",
       "999  5711                                  NaN  0.238234\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_model = ContentBasedRecommender(books, book_tags, tags)\n",
    "\n",
    "result = cb_model.get_recommendations(29703)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validating collaborative filtering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<surprise.prediction_algorithms.matrix_factorization.SVD object at 0x00000133BA74A730>\n",
      "<surprise.prediction_algorithms.slope_one.SlopeOne object at 0x00000133BA74A760>\n",
      "<surprise.prediction_algorithms.matrix_factorization.NMF object at 0x00000133BA74A820>\n",
      "<surprise.prediction_algorithms.random_pred.NormalPredictor object at 0x00000133BA74A880>\n",
      "<surprise.prediction_algorithms.knns.KNNBaseline object at 0x00000133BA74A8E0>\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "<surprise.prediction_algorithms.knns.KNNBasic object at 0x00000133BA74A970>\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "<surprise.prediction_algorithms.knns.KNNWithMeans object at 0x00000133BA74A9D0>\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "<surprise.prediction_algorithms.knns.KNNWithZScore object at 0x00000133BA74AA30>\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "<surprise.prediction_algorithms.baseline_only.BaselineOnly object at 0x00000133BA74AA90>\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "<surprise.prediction_algorithms.co_clustering.CoClustering object at 0x00000133BA74AAF0>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>0.912160</td>\n",
       "      <td>6.637595</td>\n",
       "      <td>0.465513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaselineOnly</th>\n",
       "      <td>0.912708</td>\n",
       "      <td>0.283124</td>\n",
       "      <td>0.449880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNBaseline</th>\n",
       "      <td>0.920768</td>\n",
       "      <td>2.811021</td>\n",
       "      <td>7.900410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithZScore</th>\n",
       "      <td>0.933682</td>\n",
       "      <td>8.299642</td>\n",
       "      <td>18.548539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithMeans</th>\n",
       "      <td>0.934726</td>\n",
       "      <td>6.347605</td>\n",
       "      <td>21.510130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoClustering</th>\n",
       "      <td>0.944392</td>\n",
       "      <td>5.634378</td>\n",
       "      <td>0.632011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SlopeOne</th>\n",
       "      <td>0.956437</td>\n",
       "      <td>0.388525</td>\n",
       "      <td>2.054690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF</th>\n",
       "      <td>0.980468</td>\n",
       "      <td>7.664260</td>\n",
       "      <td>0.488545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNBasic</th>\n",
       "      <td>0.992721</td>\n",
       "      <td>2.075756</td>\n",
       "      <td>7.483408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NormalPredictor</th>\n",
       "      <td>1.391827</td>\n",
       "      <td>0.258630</td>\n",
       "      <td>0.748667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 test_rmse  fit_time  test_time\n",
       "Algorithm                                      \n",
       "SVD               0.912160  6.637595   0.465513\n",
       "BaselineOnly      0.912708  0.283124   0.449880\n",
       "KNNBaseline       0.920768  2.811021   7.900410\n",
       "KNNWithZScore     0.933682  8.299642  18.548539\n",
       "KNNWithMeans      0.934726  6.347605  21.510130\n",
       "CoClustering      0.944392  5.634378   0.632011\n",
       "SlopeOne          0.956437  0.388525   2.054690\n",
       "NMF               0.980468  7.664260   0.488545\n",
       "KNNBasic          0.992721  2.075756   7.483408\n",
       "NormalPredictor   1.391827  0.258630   0.748667"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark = []\n",
    "\n",
    "for algorithm in [SVD(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
    "    # Perform cross validation\n",
    "    print(algorithm)\n",
    "    results = cross_validate(algorithm, train_data, measures=['RMSE'], cv=3, verbose=False)\n",
    "    \n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    benchmark.append(tmp)\n",
    "    \n",
    "pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')\n",
    "\n",
    "# Best perforing algorithms\n",
    "# SVD, BaselineOnly, KNNBaseline, KNNWithMeans, KNNWithZScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make recommendations for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender():\n",
    "    \n",
    "    def __init__(self, dataset, books, train_data, test_data, algorithm):\n",
    "        self.algorithm = algorithm\n",
    "        self.dataset = dataset\n",
    "        self.trainset = train_data.build_full_trainset()\n",
    "        self.train_data = train_data\n",
    "        self.testset = test_data.build_full_trainset().build_testset()\n",
    "        self.test_data = test_data\n",
    "        self.books = books\n",
    "    \n",
    "    \n",
    "    def get_unrated_items(self, user_id):\n",
    "        items = self.dataset['item'].unique()\n",
    "        items_U = self.dataset.loc[self.dataset['user'] == user_id, 'item']\n",
    "        items_to_pred = np.setdiff1d(items, items_U)\n",
    "        return items_to_pred\n",
    "    \n",
    "    \n",
    "    def rate_unrated_items(self, user_id, items_to_pred):\n",
    "        # build testset for user\n",
    "        testset_U = pd.DataFrame([[user_id, item, 4.0] for item in items_to_pred])  \n",
    "        testset_U = surprise.Dataset.load_from_df(testset_U, reader) \n",
    "        testset_U = testset_U.build_full_trainset().build_testset()\n",
    "        recommendations = self.algorithm.test(testset_U)\n",
    "\n",
    "        return recommendations\n",
    "    \n",
    "    \n",
    "    def get_Iu(self, uid):\n",
    "        \n",
    "        trainset = self.algorithm.trainset\n",
    "        try:\n",
    "            return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
    "        except ValueError: # user was not part of the trainset\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def get_Ui(self, iid):\n",
    "        \n",
    "        trainset = self.algorithm.trainset\n",
    "        try: \n",
    "            return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def get_recommendations_for_user(self, user_id):\n",
    "        \n",
    "        items_to_pred = self.get_unrated_items(user_id)\n",
    "        recommendations = self.rate_unrated_items(user_id, items_to_pred)\n",
    "        \n",
    "        df = pd.DataFrame(recommendations, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
    "        df['err'] = abs(df.est - df.rui)\n",
    "        \n",
    "        recommendations_df = df.merge(self.books, how = 'left', \n",
    "                                                  left_on = 'iid', \n",
    "                                                  right_on = 'id')[['iid', 'original_title', 'est']]\n",
    "        recommendations_df = recommendations_df.sort_values(['est'], ascending=False)\n",
    "        \n",
    "        return recommendations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_options = {'name': 'cosine',\n",
    "#                'user_based': False}\n",
    "# collab_knn = surprise.KNNBasic(k=40,sim_options=sim_options)\n",
    "# trainset = train_data.build_full_trainset()\n",
    "# collab_knn.fit(trainset)\n",
    "# testset = test_data.build_full_trainset().build_testset()\n",
    "# preds = collab_knn.test(testset)\n",
    "# surprise.accuracy.rmse(preds, verbose=True)\n",
    "# # recommender = Recommender(dataset, books, train_data, test_data, collab_knn)\n",
    "# # recommender.get_recommendations_for_user(29703)\n",
    "# # collab_knn.fit()\n",
    "# # collab_knn.get_recommendations_for_user(29703)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse_knn = []\n",
    "# sim_options = {'name': 'cosine',\n",
    "#                'user_based': False}\n",
    "# collab_knn = surprise.KNNBasic(k=40,sim_options=sim_options)\n",
    "\n",
    "# kSplit = surprise.model_selection.split.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# for trainset, testset in kSplit.split(train_data): #iterate through the folds.\n",
    "#     collab_knn.fit(trainset)\n",
    "#     preds_knn = collab_knn.test(testset)\n",
    "#     rmse_knn.append(surprise.accuracy.rmse(preds_knn,verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid recommender\n",
    "Now, we ensemble the two recommenders above:\n",
    "1. SVD\n",
    "2. BaselineOnly\n",
    "3. KNNBaseline\n",
    "4. KNNWithMeans\n",
    "5. KNNWithZScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRecommender():\n",
    "    \n",
    "    MODEL_NAME = 'Hybrid'\n",
    "    \n",
    "    def __init__(self, rs_list, wt_list, train_data, test_data):\n",
    "        \n",
    "        self.rs = rs_list \n",
    "        self.weights = wt_list\n",
    "        self.num_algo = len(rs_list)\n",
    "        self.trainset = train_data.build_full_trainset()\n",
    "        self.testset = test_data.build_full_trainset().build_testset()\n",
    "    \n",
    "    def test(self):\n",
    "        \n",
    "        df = []\n",
    "        rmse_rs = []\n",
    "        for i in range(self.num_algo):\n",
    "            self.rs[i].fit(self.trainset)\n",
    "            preds_rs = self.rs[i].test(self.testset)\n",
    "            rmse_rs.append(surprise.accuracy.rmse(preds_rs, verbose=True))\n",
    "            df.append(pd.DataFrame(preds_rs, columns=['uid', 'iid', 'rui', 'est', 'details']))\n",
    "            \n",
    "        test_df = pd.DataFrame(self.testset, columns=['user', 'item', 'rating'])\n",
    "\n",
    "        preds_net = 0\n",
    "        for i in range(self.num_algo):\n",
    "            preds_net += self.weights[i]*df[i]['est']\n",
    "        preds_net /= sum(self.weights)\n",
    "        \n",
    "        rmse_net = mean_squared_error(test_df['rating'], preds_net, squared=False)\n",
    "   \n",
    "        return rmse_rs, rmse_net\n",
    "        \n",
    "    \n",
    "    def recommend_items(self, user_id, items_to_ignore=[], topn=10, verbose=False):\n",
    "        \n",
    "        #Ignores items the user has already interacted --- LATER  \n",
    "                \n",
    "        # get the top-1000 cb_model recommendations\n",
    "        cb_recs = self.cb_model.get_recommendations(user_id)\n",
    "        \n",
    "        # get the top-1000 collab_knn recommendation\n",
    "        knn_recs = self.collab_knn.get_recommendations_for_user(user_id)\n",
    "        \n",
    "        # combine results by id\n",
    "        recomm = cb_recs.merge(knn_recs,\n",
    "                                   how = 'outer', \n",
    "                                   left_on = 'id', \n",
    "                                   right_on = 'iid').fillna(0.0)\n",
    "        \n",
    "        # compute a HYBRID recommendation score based on knn and svd\n",
    "        recomm['score_hybrid'] = (recomm['rating'] * self.cb_ensemble_weight) + (recomm['est'] * self.knn_ensemble_weight)\n",
    "                                    \n",
    "        \n",
    "        recommendations = recomm.sort_values('score_hybrid', ascending=False).head(topn)\n",
    "        \n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8901\n",
      "Estimating biases using als...\n",
      "RMSE: 0.8940\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8834\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8946\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8953\n"
     ]
    }
   ],
   "source": [
    "seed = 6\n",
    "svd = SVD(random_state=seed)\n",
    "baseline = BaselineOnly()\n",
    "knn_baseline = KNNBaseline(random_state=seed)\n",
    "knn_means = KNNWithMeans(random_state=seed)\n",
    "knn_zscore = KNNWithZScore(random_state=seed)\n",
    "cocluster = CoClustering(random_state=seed)\n",
    "\n",
    "rs_list = [svd, baseline, knn_baseline, knn_means, knn_zscore]\n",
    "wt_list = [0.7, 0.7, 1, 0.5, 0.5]\n",
    "\n",
    "hybrid_rs = HybridRecommender(rs_list, wt_list, train_data, test_data)\n",
    "r, rn = hybrid_rs.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE list :  [0.8901423538839612, 0.8940185152048475, 0.8834221127061234, 0.8946312525767542, 0.8952990172970682]\n",
      "RMSE net :  0.8769674027946868\n"
     ]
    }
   ],
   "source": [
    "print('RMSE list : ', r)\n",
    "print('RMSE net : ', rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
