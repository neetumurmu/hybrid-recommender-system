{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import surprise\n",
    "from sklearn.model_selection import train_test_split#, cross_validate\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import KNNWithMeans, SVD, SVDpp, SlopeOne\n",
    "from surprise import accuracy\n",
    "from sklearn.preprocessing import normalize\n",
    "import scipy\n",
    "from surprise import NormalPredictor\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNWithZScore\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD\n",
    "from surprise import BaselineOnly\n",
    "from surprise import SVDpp\n",
    "from surprise import NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv('goodbooks-10k/books.csv')\n",
    "ratings = pd.read_csv('goodbooks-10k/ratings.csv')\n",
    "toread = pd.read_csv('goodbooks-10k/to_read.csv')\n",
    "tags = pd.read_csv('goodbooks-10k/tags.csv')\n",
    "book_tags = pd.read_csv('goodbooks-10k/book_tags.csv')\n",
    "\n",
    "\n",
    "# pint(books.shape, '\\n', books.head())\n",
    "# print(ratings.shape, '\\n', ratings.head())\n",
    "# print(toread.shape, '\\n', toread.head())\n",
    "# print(tags.shape, '\\n', tags.tail())\n",
    "# print(book_tags.shape , '\\n',  book_tags.head())\n",
    "\n",
    "# len(ratings['user_id'].unique()), len(ratings['book_id'].unique())\n",
    "# ratings[ratings['user_id']==50]\n",
    "\n",
    "# print('Before : ', ratings.shape[0])\n",
    "# ratings.drop_duplicates(inplace=True)\n",
    "# print('After dropping duplicates: ', ratings.shape[0])\n",
    "# print('The number of unique users we have is:', len(ratings.user_id.unique()))\n",
    "# print('The number of unique books we have is:', len(ratings.book_id.unique()))\n",
    "# print(\"The median user rated %d books.\"%ratings.user_id.value_counts().median())\n",
    "# print('The max rating is: %d,'%ratings.rating.max(),\" and the min rating is: %d\"%ratings.rating.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend books based on authors\n",
    "\n",
    "##### Concatenate (first name + last name) of author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merge 'tags' and 'book_tags' \n",
    "# tags_joined = pd.merge(book_tags, tags, left_on='tag_id', right_on='tag_id', how='inner')\n",
    "\n",
    "# # item_ids\n",
    "# item_ids = books['book_id'].tolist()\n",
    "\n",
    "# # author to feature\n",
    "\n",
    "# tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
    "# tfidf_matrix = tf.fit_transform(books['authors'].head(10000))\n",
    "# cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# # Build a 1-d array with book titles\n",
    "\n",
    "# titles = books['title']\n",
    "# indices = pd.Series(books.index, index=books['title'])\n",
    "\n",
    "# # Get book recommendation based on cosine similarity\n",
    "# def authors_recommendations(title):\n",
    "#     idx = indices[title]\n",
    "#     sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "#     sim_scores = sorted(sim_scores, key=lambda x : x[1], reverse=True)\n",
    "#     sim_scores = sim_scores[1:21]\n",
    "#     book_indices = [i[0] for i in sim_scores]\n",
    "#     return titles.iloc[book_indices]\n",
    "\n",
    "# result = authors_recommendations('The Hobbit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ratings[['user_id','book_id','rating']]\n",
    "dataset.columns = ['user','item','rating']\n",
    "dataset = dataset[0:100000]\n",
    "\n",
    "train_split, test_split = train_test_split(dataset, test_size=0.25)\n",
    "\n",
    "reader = surprise.Reader(rating_scale=(1,5))\n",
    "train_data = surprise.Dataset.load_from_df(train_split, reader)\n",
    "test_data = surprise.Dataset.load_from_df(test_split, reader)\n",
    "\n",
    "rmse_knn = []\n",
    "rmse_cb = []\n",
    "\n",
    "# collab_knn.get_neighbors(50, 5)    # .get_neighbors() returns closest items to given arg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build user profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_profile(item_id, item_ids, tfidf_matrix):\n",
    "    idx = item_ids.index(item_id)\n",
    "    item_profile = tfidf_matrix[idx:idx+1]\n",
    "    return item_profile\n",
    "\n",
    "\n",
    "def get_item_profiles(ids, item_ids, tfidf_matrix):\n",
    "    item_profiles_list = [get_item_profile(x, item_ids, tfidf_matrix) for x in ids]\n",
    "    item_profiles = scipy.sparse.vstack(item_profiles_list)\n",
    "    return item_profiles\n",
    "\n",
    "\n",
    "def build_users_profile(user_id, train_indexed, item_ids, tfidf_matrix):\n",
    "    train_user_df = train_indexed.loc[user_id]\n",
    "    user_item_profiles = get_item_profiles(pd.Series(train_user_df['item']), item_ids, tfidf_matrix)\n",
    "    user_item_strengths = np.array(train_user_df['rating']).reshape(-1,1)\n",
    "    \n",
    "    #Weighted average of item profiles by the interactions strength\n",
    "    user_item_strengths_weighted_avg = np.sum(user_item_profiles.multiply(user_item_strengths), axis=0) / np.sum(user_item_strengths)\n",
    "    user_profile_norm = normalize(user_item_strengths_weighted_avg)\n",
    "    return user_profile_norm\n",
    "\n",
    "\n",
    "def build_users_profiles(item_ids, tfidf_matrix): \n",
    "    train_indexed = train_split[train_split['item'].isin(books.index+1)].set_index('user')\n",
    "    user_profiles = {}\n",
    "    for user_id in train_indexed.index.unique():\n",
    "        user_profiles[user_id] = build_users_profile(user_id, train_indexed, item_ids, tfidf_matrix)\n",
    "        \n",
    "    return user_profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommend books based on tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedRecommender():\n",
    "    \n",
    "    def __init__(self, books, book_tags, tags):\n",
    "        self.model_name = 'Content-Based-Recommender'\n",
    "        self.books = books\n",
    "        self.titles = books['title']\n",
    "        self.indices = pd.Series(books.index, index=books['title'])\n",
    "        self.tags_joined = pd.merge(book_tags, tags, \n",
    "                                    left_on='tag_id', right_on='tag_id', how='inner')\n",
    "        self.books_with_tags = pd.merge(books, self.tags_joined, \n",
    "                                        left_on='book_id', right_on='goodreads_book_id', how='inner')\n",
    "        self.item_ids  = self.get_item_ids()\n",
    "        self.tfidf_matrix = self.get_tfidf()\n",
    "        self.user_profiles = build_users_profiles(self.item_ids, self.tfidf_matrix)\n",
    "\n",
    "    \n",
    "    def get_item_ids(self):\n",
    "        self.item_ids = (self.books['id']).tolist()\n",
    "        return self.item_ids\n",
    "    \n",
    "    \n",
    "    def get_tfidf(self):\n",
    "        tf = TfidfVectorizer(analyzer='word',\n",
    "                             ngram_range=(1, 2),\n",
    "                             min_df=0,\n",
    "                             max_features=1000,\n",
    "                             stop_words='english')\n",
    "        tfidf_matrix = tf.fit_transform(self.books_with_tags['tag_name'].head(10000))\n",
    "        return tfidf_matrix\n",
    "    \n",
    "    \n",
    "    def get_cosine_similarity(self, tfidf_matrix1, tfidf_matrix2):\n",
    "        print(tfidf_matrix1.shape)  #user_profile\n",
    "        print(tfidf_matrix2.shape)  #base tfidf\n",
    "        cosine_sim = linear_kernel(tfidf_matrix1, tfidf_matrix2)\n",
    "        \n",
    "        return cosine_sim\n",
    "    \n",
    "    \n",
    "    def _get_similar_items_to_user_profile(self, user_id, topn=1000):\n",
    "        cosine_similarities = self.get_cosine_similarity(self.user_profiles[user_id], self.tfidf_matrix)\n",
    "        similar_indices = cosine_similarities.argsort().flatten()[-topn:]\n",
    "        similar_items = sorted([(self.item_ids[i], cosine_similarities[0,i]) for i in similar_indices], \n",
    "                               key=lambda x: -x[1])\n",
    "        return similar_items\n",
    "    \n",
    "    \n",
    "    def get_recommendations(self, user_id, topn=1000):\n",
    "        similar_items = self._get_similar_items_to_user_profile(user_id)\n",
    "        \n",
    "        #Ignores items the user has already interacted --- LATER\n",
    "        \n",
    "        recommendations_df = pd.DataFrame(similar_items, columns=['book_id', 'rating']).head(topn)\n",
    "        recommendations_df = recommendations_df.merge(self.books, how = 'left', \n",
    "                                                          left_on = 'book_id', \n",
    "                                                          right_on = 'id')[['id', 'original_title', 'rating']]\n",
    "        return recommendations_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cb_model = ContentBasedRecommender(books, book_tags, tags)\n",
    "\n",
    "# result = cb_model.get_recommendations(29703)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validating collaborative filtering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<surprise.prediction_algorithms.matrix_factorization.SVD object at 0x0000026B0DEC95B0>\n",
      "<surprise.prediction_algorithms.slope_one.SlopeOne object at 0x0000026B0DE18CA0>\n",
      "<surprise.prediction_algorithms.matrix_factorization.NMF object at 0x0000026B0DE05460>\n",
      "<surprise.prediction_algorithms.random_pred.NormalPredictor object at 0x0000026B0DD605E0>\n",
      "<surprise.prediction_algorithms.knns.KNNBaseline object at 0x0000026B0DBF8C40>\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "<surprise.prediction_algorithms.knns.KNNBasic object at 0x0000026B0DCE4A60>\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "<surprise.prediction_algorithms.knns.KNNWithMeans object at 0x0000026B0DEB4C70>\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "<surprise.prediction_algorithms.knns.KNNWithZScore object at 0x0000026B219EDC40>\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "<surprise.prediction_algorithms.baseline_only.BaselineOnly object at 0x0000026B219ED220>\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "<surprise.prediction_algorithms.co_clustering.CoClustering object at 0x0000026B219ED340>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_rmse</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>0.911299</td>\n",
       "      <td>6.615524</td>\n",
       "      <td>0.468815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaselineOnly</th>\n",
       "      <td>0.912511</td>\n",
       "      <td>0.231861</td>\n",
       "      <td>0.325903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNBaseline</th>\n",
       "      <td>0.920036</td>\n",
       "      <td>1.672253</td>\n",
       "      <td>6.355297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithMeans</th>\n",
       "      <td>0.928655</td>\n",
       "      <td>2.189351</td>\n",
       "      <td>6.003163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNWithZScore</th>\n",
       "      <td>0.929752</td>\n",
       "      <td>2.150114</td>\n",
       "      <td>6.427357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoClustering</th>\n",
       "      <td>0.942115</td>\n",
       "      <td>4.393441</td>\n",
       "      <td>0.521624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SlopeOne</th>\n",
       "      <td>0.951473</td>\n",
       "      <td>0.361955</td>\n",
       "      <td>1.812613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NMF</th>\n",
       "      <td>0.978210</td>\n",
       "      <td>7.342017</td>\n",
       "      <td>0.489212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNNBasic</th>\n",
       "      <td>0.990543</td>\n",
       "      <td>1.406821</td>\n",
       "      <td>5.508454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NormalPredictor</th>\n",
       "      <td>1.392207</td>\n",
       "      <td>0.197889</td>\n",
       "      <td>0.446838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 test_rmse  fit_time  test_time\n",
       "Algorithm                                      \n",
       "SVD               0.911299  6.615524   0.468815\n",
       "BaselineOnly      0.912511  0.231861   0.325903\n",
       "KNNBaseline       0.920036  1.672253   6.355297\n",
       "KNNWithMeans      0.928655  2.189351   6.003163\n",
       "KNNWithZScore     0.929752  2.150114   6.427357\n",
       "CoClustering      0.942115  4.393441   0.521624\n",
       "SlopeOne          0.951473  0.361955   1.812613\n",
       "NMF               0.978210  7.342017   0.489212\n",
       "KNNBasic          0.990543  1.406821   5.508454\n",
       "NormalPredictor   1.392207  0.197889   0.446838"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark = []\n",
    "\n",
    "# Iterate over all algorithms\n",
    "# for algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
    "\n",
    "for algorithm in [SVD(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
    "    # Perform cross validation\n",
    "    print(algorithm)\n",
    "    results = cross_validate(algorithm, train_data, measures=['RMSE'], cv=3, verbose=False)\n",
    "    \n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    benchmark.append(tmp)\n",
    "    \n",
    "pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')\n",
    "\n",
    "# Best perforing algorithms\n",
    "# SVD, BaselineOnly, KNNBaseline, KNNWithMeans, KNNWithZScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make recommendations for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recommender():\n",
    "    \n",
    "    def __init__(self, dataset, books, train_data, test_data, algorithm):\n",
    "        self.algorithm = algorithm\n",
    "        self.dataset = dataset\n",
    "        self.trainset = train_data.build_full_trainset()\n",
    "        self.train_data = train_data\n",
    "        self.testset = test_data.build_full_trainset().build_testset()\n",
    "        self.test_data = test_data\n",
    "        self.books = books\n",
    "    \n",
    "    \n",
    "    def get_unrated_items(self, user_id):\n",
    "        items = self.dataset['item'].unique()\n",
    "        items_U = self.dataset.loc[self.dataset['user'] == user_id, 'item']\n",
    "        items_to_pred = np.setdiff1d(items, items_U)\n",
    "        return items_to_pred\n",
    "    \n",
    "    \n",
    "    def rate_unrated_items(self, user_id, items_to_pred):\n",
    "        # build testset for user\n",
    "        testset_U = pd.DataFrame([[user_id, item, 4.0] for item in items_to_pred])  \n",
    "        testset_U = surprise.Dataset.load_from_df(testset_U, reader) \n",
    "        testset_U = testset_U.build_full_trainset().build_testset()\n",
    "        recommendations = self.algorithm.test(testset_U)\n",
    "\n",
    "        return recommendations\n",
    "    \n",
    "    \n",
    "    def get_Iu(self, uid):\n",
    "        \n",
    "        trainset = self.algorithm.trainset\n",
    "        try:\n",
    "            return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
    "        except ValueError: # user was not part of the trainset\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def get_Ui(self, iid):\n",
    "        \n",
    "        trainset = self.algorithm.trainset\n",
    "        try: \n",
    "            return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def get_recommendations_for_user(self, user_id):\n",
    "        \n",
    "        items_to_pred = self.get_unrated_items(user_id)\n",
    "        recommendations = self.rate_unrated_items(user_id, items_to_pred)\n",
    "        \n",
    "        df = pd.DataFrame(recommendations, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
    "        df['err'] = abs(df.est - df.rui)\n",
    "        \n",
    "        recommendations_df = df.merge(self.books, how = 'left', \n",
    "                                                  left_on = 'iid', \n",
    "                                                  right_on = 'id')[['iid', 'original_title', 'est']]\n",
    "        recommendations_df = recommendations_df.sort_values(['est'], ascending=False)\n",
    "        \n",
    "        return recommendations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_options = {'name': 'cosine',\n",
    "#                'user_based': False}\n",
    "# collab_knn = surprise.KNNBasic(k=40,sim_options=sim_options)\n",
    "# trainset = train_data.build_full_trainset()\n",
    "# collab_knn.fit(trainset)\n",
    "# testset = test_data.build_full_trainset().build_testset()\n",
    "# preds = collab_knn.test(testset)\n",
    "# surprise.accuracy.rmse(preds, verbose=True)\n",
    "# # recommender = Recommender(dataset, books, train_data, test_data, collab_knn)\n",
    "# # recommender.get_recommendations_for_user(29703)\n",
    "# # collab_knn.fit()\n",
    "# # collab_knn.get_recommendations_for_user(29703)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse_knn = []\n",
    "# sim_options = {'name': 'cosine',\n",
    "#                'user_based': False}\n",
    "# collab_knn = surprise.KNNBasic(k=40,sim_options=sim_options)\n",
    "\n",
    "# kSplit = surprise.model_selection.split.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# for trainset, testset in kSplit.split(train_data): #iterate through the folds.\n",
    "#     collab_knn.fit(trainset)\n",
    "#     preds_knn = collab_knn.test(testset)\n",
    "#     rmse_knn.append(surprise.accuracy.rmse(preds_knn,verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling\n",
    "Now, we ensemble the two recommenders above:\n",
    "1. SVD\n",
    "2. BaselineOnly\n",
    "3. KNNBaseline\n",
    "4. KNNWithMeans\n",
    "5. KNNWithZScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRecommender():\n",
    "    \n",
    "    MODEL_NAME = 'Hybrid'\n",
    "    \n",
    "    def __init__(self, rs_list, wt_list, train_data, test_data):\n",
    "        \n",
    "        self.rs = rs_list \n",
    "        self.weights = wt_list\n",
    "        self.num_algo = len(rs_list)\n",
    "        self.trainset = train_data.build_full_trainset()\n",
    "        self.testset = test_data.build_full_trainset().build_testset()\n",
    "    \n",
    "    def test(self):\n",
    "        \n",
    "        df = []\n",
    "        rmse_rs = []\n",
    "        for i in range(self.num_algo):\n",
    "            self.rs[i].fit(self.trainset)\n",
    "            preds_rs = self.rs[i].test(self.testset)\n",
    "            rmse_rs.append(surprise.accuracy.rmse(preds_rs, verbose=True))\n",
    "            df.append(pd.DataFrame(preds_rs, columns=['uid', 'iid', 'rui', 'est', 'details']))\n",
    "            \n",
    "        test_df = pd.DataFrame(self.testset, columns=['user', 'item', 'rating'])\n",
    "\n",
    "        preds_net = 0\n",
    "        for i in range(self.num_algo):\n",
    "            preds_net += self.weights[i]*df[i]['est']\n",
    "        preds_net /= sum(self.weights)\n",
    "        \n",
    "        rmse_net = mean_squared_error(test_df['rating'], preds_net, squared=False)\n",
    "   \n",
    "        return rmse_rs, rmse_net\n",
    "        \n",
    "    \n",
    "    def recommend_items(self, user_id, items_to_ignore=[], topn=10, verbose=False):\n",
    "        \n",
    "        #Ignores items the user has already interacted --- LATER  \n",
    "                \n",
    "        # get the top-1000 cb_model recommendations\n",
    "        cb_recs = self.cb_model.get_recommendations(user_id)\n",
    "        \n",
    "        # get the top-1000 collab_knn recommendation\n",
    "        knn_recs = self.collab_knn.get_recommendations_for_user(user_id)\n",
    "        \n",
    "        # combine results by id\n",
    "        recomm = cb_recs.merge(knn_recs,\n",
    "                                   how = 'outer', \n",
    "                                   left_on = 'id', \n",
    "                                   right_on = 'iid').fillna(0.0)\n",
    "        \n",
    "        # compute a HYBRID recommendation score based on knn and svd\n",
    "        recomm['score_hybrid'] = (recomm['rating'] * self.cb_ensemble_weight) + (recomm['est'] * self.knn_ensemble_weight)\n",
    "                                    \n",
    "        \n",
    "        recommendations = recomm.sort_values('score_hybrid', ascending=False).head(topn)\n",
    "        \n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8902\n",
      "Estimating biases using als...\n",
      "RMSE: 0.8936\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.8840\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9005\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9002\n"
     ]
    }
   ],
   "source": [
    "seed = 6\n",
    "svd = SVD(random_state=seed)\n",
    "baseline = BaselineOnly()\n",
    "knn_baseline = KNNBaseline(random_state=seed)\n",
    "knn_means = KNNWithMeans(random_state=seed)\n",
    "knn_zscore = KNNWithZScore(random_state=seed)\n",
    "cocluster = CoClustering(random_state=seed)\n",
    "\n",
    "rs_list = [svd, baseline, knn_baseline, knn_means, knn_zscore]\n",
    "wt_list = [0.7, 0.7, 1, 0.5, 0.5]\n",
    "\n",
    "hybrid_rs = HybridRecommender(rs_list, wt_list, train_data, test_data)\n",
    "r, rn = hybrid_rs.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE list :  [0.8902136543637234, 0.893594918712878, 0.8839550444010758, 0.9005142246712313, 0.900156408865128]\n",
      "RMSE net :  0.8778993451319257\n"
     ]
    }
   ],
   "source": [
    "print('RMSE list : ', r)\n",
    "print('RMSE net : ', rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3999999999999995"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(wt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
